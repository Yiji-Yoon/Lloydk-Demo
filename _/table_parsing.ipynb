{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4390de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfitz\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from bs4 import BeautifulSoup\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "upstage_api_key = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(input_file, batch_size):\n",
    "    # Open input_pdf\n",
    "    input_pdf = fitz.open(input_file)\n",
    "    num_pages = len(input_pdf)\n",
    "    print(f\"Total number of pages: {num_pages}\")\n",
    " \n",
    "    # Split input_pdf\n",
    "    for start_page in range(0, num_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, num_pages) - 1\n",
    " \n",
    "        # Write output_pdf to file\n",
    "        input_file_basename = os.path.splitext(input_file)[0]\n",
    "        output_file = f\"{input_file_basename}_{start_page}_{end_page}.pdf\"\n",
    "        print(output_file)\n",
    "        with fitz.open() as output_pdf:\n",
    "            output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)\n",
    "            output_pdf.save(output_file)\n",
    " \n",
    "    # Close input_pdf\n",
    "    input_pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a0799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 37\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_0_0.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_1_1.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_2_2.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_3_3.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_4_4.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_5_5.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_6_6.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_7_7.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_8_8.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_9_9.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_10_10.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_11_11.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_12_12.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_13_13.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_14_14.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_15_15.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_16_16.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_17_17.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_18_18.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_19_19.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_20_20.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_21_21.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_22_22.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_23_23.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_24_24.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_25_25.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_26_26.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_27_27.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_28_28.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_29_29.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_30_30.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_31_31.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_32_32.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_33_33.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_34_34.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_35_35.pdf\n",
      "/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_36_36.pdf\n"
     ]
    }
   ],
   "source": [
    "# Input arguments\n",
    "input_file = \"/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정.pdf\" # Replace with a file of your own\n",
    "batch_size = 1  # Maximum available value is 100\n",
    "split_pdf(input_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a47189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블을 구분하는 코드\n",
    "\n",
    "tables = []\n",
    "paragraphs = []\n",
    "\n",
    "for i in range(37):\n",
    "    filename = f\"/home/admin/Desktop/Demo/_/files/LLOYDK_백서_docs_지속수정_{i}_{i}.pdf\"\n",
    "    url = \"https://api.upstage.ai/v1/document-digitization\"\n",
    "    headers = {\"Authorization\": f\"Bearer {upstage_api_key}\"}\n",
    "    files = {\"document\": open(filename, \"rb\")}\n",
    "    data = {\"ocr\": \"force\", \"base64_encoding\": \"['table']\", \"model\": \"document-parse\"}\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    for element in response.json()['elements']:\n",
    "        if element['category'] == 'table':\n",
    "            table_html = element['content'][\"html\"]\n",
    "            soup = BeautifulSoup(table_html, \"html.parser\")\n",
    "            # rows = []\n",
    "            for tr in soup.find_all(\"tr\"):\n",
    "                cells = [\n",
    "                    td.get_text(strip=True).replace(\"\\n\", \" \")\n",
    "                    for td in tr.find_all([\"td\", \"th\"])\n",
    "                ]\n",
    "                paragraphs.append(cells)\n",
    "            # for r in rows:\n",
    "            #     paragraphs.append(r)\n",
    "        else:\n",
    "            html_str = element['content']['html']\n",
    "            if html_str.startswith('\"') and html_str.endswith('\"'):\n",
    "                html_str = html_str[1:-1]\n",
    "            soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=False)\n",
    "            paragraphs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f64028",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []\n",
    "current_type = None   # 'text' 혹은 'table'\n",
    "current_items = []\n",
    "\n",
    "def item_type(item):\n",
    "    # 문자열이면 문장\n",
    "    if isinstance(item, str):\n",
    "        return \"text\"\n",
    "    # 리스트면 (여기서는) 테이블\n",
    "    if isinstance(item, list):\n",
    "        return \"table\"\n",
    "    return \"other\"\n",
    "\n",
    "for item in paragraphs:\n",
    "    t = item_type(item)\n",
    "\n",
    "    # text / table / other 타입이 바뀌면 이전 블록을 마무리\n",
    "    if t != current_type:\n",
    "        if current_items:\n",
    "            blocks.append({\n",
    "                \"type\": current_type,\n",
    "                \"items\": current_items\n",
    "            })\n",
    "        current_type = t\n",
    "        current_items = []\n",
    "\n",
    "    current_items.append(item)\n",
    "\n",
    "# 마지막 블록도 추가\n",
    "if current_items:\n",
    "    blocks.append({\n",
    "        \"type\": current_type,\n",
    "        \"items\": current_items\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f42891",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_embedding = []\n",
    "for i in range(len(blocks)):\n",
    "    texts = []\n",
    "    if blocks[i]['type'] == 'text':\n",
    "        texts.append(blocks[i]['items'])\n",
    "        text_for_embedding.append(texts[0])\n",
    "    else:\n",
    "        texts = []\n",
    "        texts.append(blocks[i-1]['items'][-1:]+blocks[i]['items'])\n",
    "        text_for_embedding.append(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a855a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_char_with_overlap(lines, max_chars=1500, overlap_chars=70):\n",
    "    \"\"\"\n",
    "    lines: 이미 문장 단위로 나뉜 리스트 (list[str])\n",
    "    max_chars: 청크 하나의 최대 글자 수\n",
    "    overlap_chars: 다음 청크로 넘길 최소 글자 수 (겹치는 분량)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current = []\n",
    "    cur_len = 0\n",
    "\n",
    "    for raw in lines:\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        line_len = len(line)\n",
    "\n",
    "        # 이 줄을 더하면 max_chars를 넘는 경우 → 지금까지 걸 하나의 청크로 확정\n",
    "        if current and (cur_len + line_len > max_chars):\n",
    "            # 현재 청크 저장\n",
    "            chunks.append(\"\\n\".join(current))\n",
    "\n",
    "            # 🔁 오버랩 부분 만들기: 뒤에서부터 overlap_chars 이상이 될 때까지 가져오기\n",
    "            overlap = []\n",
    "            overlap_len = 0\n",
    "            for s in reversed(current):\n",
    "                if overlap_len + len(s) > overlap_chars and overlap:\n",
    "                    break\n",
    "                overlap.append(s)\n",
    "                overlap_len += len(s)\n",
    "            overlap = list(reversed(overlap))\n",
    "\n",
    "            current = overlap[:]          # 새 청크는 오버랩으로 시작\n",
    "            cur_len = sum(len(s) for s in current)\n",
    "\n",
    "        # 현재 청크에 이 줄 추가\n",
    "        current.append(line)\n",
    "        cur_len += line_len\n",
    "\n",
    "    # 마지막 청크 처리\n",
    "    if current:\n",
    "        chunks.append(\"\\n\".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94e484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_chunks = []\n",
    "for chunks in text_for_embedding:\n",
    "    if type(chunks[1]) == list:\n",
    "        list_for_chunks.append(chunks)\n",
    "        continue\n",
    "    else:\n",
    "        num_chunks = chunk_by_char_with_overlap(chunks)\n",
    "        for num in num_chunks:\n",
    "            list_for_chunks.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5de9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant 연결 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 설정 (Qwen)\n",
    "EMBED_MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "EMBED_DIM = 1024  # Qwen은 1024차원 (중요!)\n",
    "COLLECTION_NAME = \"lloydk_docs\"\n",
    "\n",
    "# 2. 임베딩 모델 로드 (실제 변환을 담당할 녀석)\n",
    "# (아까 위에서 이미 로드했다면 이 줄은 생략해도 됩니다)\n",
    "model = SentenceTransformer(EMBED_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# 3. Qdrant 클라이언트 연결 (Docker)\n",
    "# Docker가 켜져 있어야 접속됩니다!\n",
    "client_qd = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "print(\"Qdrant 연결 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b963c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking collection existence...\n",
      "→ Exists? False\n",
      "✅ Created collection: lloydk_docs\n"
     ]
    }
   ],
   "source": [
    "def ensure_collection():\n",
    "    print(\"🔍 Checking collection existence...\")\n",
    "    exists = client_qd.collection_exists(collection_name=COLLECTION_NAME)\n",
    "    print(f\"→ Exists? {exists}\")\n",
    "\n",
    "    if not exists:\n",
    "        client_qd.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"✅ Created collection: {COLLECTION_NAME}\")\n",
    "    else:\n",
    "        print(f\"⚙️ Collection already exists: {COLLECTION_NAME}\")\n",
    "        \n",
    "ensure_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a4f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_batch(texts):\n",
    "    \"\"\"\n",
    "    texts: list[str]\n",
    "    return: vectors(list[list[float]]), cleaned_texts(list[str])\n",
    "    \"\"\"\n",
    "    # 1. 빈 문자열 제거 + 전처리 (작성하신 것 그대로 유지)\n",
    "    cleaned = []\n",
    "    \n",
    "    for t in texts:\n",
    "        if not isinstance(t, str):\n",
    "            t = str(t)\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        cleaned.append(t)\n",
    "\n",
    "    # 데이터가 없으면 빈 리스트 반환\n",
    "    if not cleaned:\n",
    "        return [], []\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. 여기가 핵심 변경 포인트! (OpenAI -> Qwen)\n",
    "    # ---------------------------------------------------------\n",
    "    # model.encode()는 SentenceTransformer의 기본 함수입니다.\n",
    "    # normalize_embeddings=True: 벡터 크기를 정규화 (검색 성능 향상에 좋음)\n",
    "    embeddings = model.encode(cleaned, normalize_embeddings=True)\n",
    "\n",
    "    # numpy array로 나오기 때문에, 리스트 형태로 변환(.tolist())\n",
    "    vectors = embeddings.tolist()\n",
    "    \n",
    "    return vectors, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14aa25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "for i in range(0, len(list_for_chunks), BATCH_SIZE):\n",
    "    batch = list_for_chunks[i:i+BATCH_SIZE]\n",
    "    vectors, cleaned = embed_batch(batch)\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=vec,\n",
    "            payload={\"text\": text},\n",
    "        )\n",
    "        for text, vec in zip(cleaned, vectors)\n",
    "    ]\n",
    "\n",
    "    client_qd.upsert(collection_name=\"lloydk_docs\", points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbad25cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.451\n",
      "text: ['Q. 일반 관계형 데이터 베이스에서의 데이터셋과 온톨로지와의 차이는 무엇인가요?', ['기능', '팔란티어 온톨로지', 'RDBMS'], ['데이터 모델', '객체-관계 중심', '테이블-열 중심'], ['스키마', '동적 변경 가능', '고정된 구조'], ['추론 능력', '규칙 기반 자동 추론', 'SQL 쿼리 수동 작성'], ['보안', '객체(Object) 단위 세밀한 접근 제어', '테이블 단위 권한']]\n",
      "\n",
      "score: 0.417\n",
      "text: ['A. 대표적인 LLM 모델들의 파라미터 수와 컨텍스트 윈도우 길이', ['모델명', '파라미터 수', '컨텍스트 윈도우', '특징', '적합한 용도'], ['Llama 4 (Maverick)', '400B', '1M', '초대형 다국어 대응, 멀티모달 처리', '글로벌 대용량 문서 분석, RAG'], ['DeepSeek R1', '671B (MoE)', '128K', '추론 특화, 비용 효율', '고난도 질문 응답, 분석 도구'], ['Claude 3.7 Sonnet', '(비공개)', '350K', '안전성과 판단력 강화', '챗봇, 문서 정리, 비서형 AI'], ['Mistral Small 3.1', '24B', '128K', '가볍고 빠름, 오픈소스', '내부 시스템 연동, 엣지 디바이스'], ['Phi-4', '14.7B', '16K', '저사양 환경 대응', '임베디드 AI, 비용 민감 환경'], ['GPT-4o', '1.8T (추정)', '2M', '텍스트+이미지 동시 처리', '최고 성능, 복합 AI 서비스'], ['XGen-7B', '7B', '8K', '중소규모 조직용', '문서 요약, 간단 질의응답']]\n",
      "\n",
      "score: 0.390\n",
      "text: Q. 자체 벡터 검색 엔진이 있는 건가요, 아니면 엘라스틱서치 기반인가요?\n",
      "A. 엘라스틱서치를 기반으로 하되, 리트리버, 벡터 기반 서치, 리랭킹 기술과 관련한 자사 코드 기반 기술 및 노하우 등을 조합해 적용함. 프로젝트에 따라 알맞는 벡터 모델(M3, e5 등)을 기술 조사 및 검증을 통해해 적절히 선택하여 활용 중입니다.\n",
      "6. 보안 및 개인정보 처리\n",
      "#PII #비식별화 #암호화\n",
      "6.1 민감 정보 처리 및 보안\n",
      "Q. 검색 정보에 개인정보나 금융정보 처리는 어떻게 하는지?\n",
      "A. PII(개인정보 식별자) 마스킹 및 비식별화 처리 후 검색 및 응답 제어 기능을 제공합니다. 데이터 수집단계에서 개인정보를 정규표현 및 패턴 기반으로 찾아서 마스킹 진행 하며 프롬프트에서 주요 개인 정보 금융정보를 노출 하지 않도록 프롬프트 엔지니어링을 적용합니다. 사후에도 프롬프트 및 조회 로그를 기반으로 개인정보 금융정보가 노출 되었는지 검증하는 방식으로 사업 진행을 하고 있습니다.\n",
      "사업 규모 및 보안에 민감성에 따라서 전문 보안 솔루션(예시. 구간암호화, 데이터 비식별화 솔루션)과의 연계를 진행 하기도 합니다.\n",
      "Q. 보안 공격 대응 (프롬프트 인젝션 등)은 가능한가?\n",
      "A. 권한 기반 답변 제한, 프롬프트 패턴 필터링 등의 대응책이 포함되어 있습니다..\n",
      "Q. LLM과 통신 구간에 대한 보안 처리는 어떤 방식이 있는가?\n",
      "A. 앞에 서비스 단계 WEB / WAS 단계 통신에 대한 암호화 및 권한, 인증 통제등으로 보안을 강화하고 데이터에서 민감정보에 대한 표현을 통제 하는 방식으로 하고 있습니다.\n",
      "6.2 문서 및 접근 권한 제어\n",
      "#권한제어 #RBAC #버전관리 #문서보안\n",
      "Q. 전자 문서들에 대한 검색 권한 반영한 검색 처리는 어떻게 하는지?\n",
      "A. 수집 단계에서 문서의 권한 관리 인덱스를 별도로 구성합니다.\n",
      "조회 할 때 자주 변경되고 업데이트 되는 권한을 감안하여 처리하는 부분은 성능이 매우 떨어지는 부분이 있었습니다. 권한 인덱스에서 권한 처리를 먼저 조회 확인 후 데이터를 조회해서 가저오는 두번 쿼리 하는 방식으로적용하는것이 경험상 가장 잘 되어서, 삼성전자에서도 이러한 방식 적용하여 프로젝트를 진행하였습니다\n",
      "Q. RAG 시스템 내 권한 관리는 어떤 식으로 이루어지는가? (관리자, 사용자)\n",
      "A. RAG 시스템에서는 RBAC, 즉 역할 기반 접근 제어 방식을 채택하여 권한을 관리합니다.\n",
      "이 방식에서는 권한을 사용자에게 직접 부여하지 않고, 역할 단위로 먼저 권한을 정의한 다음, 사용자가 해당 역할을 갖도록 설정합니다. 예를 들어, 관리자 역할에는 문서 색인, 삭제, 로그 조회 등의 권한이 포함되고, 일반 사용자 역할에는 문서 검색 및 조회만 포함됩니다. 각 사용자는 하나 이상의 역할을 가질 수 있으며, 역할 변경 시 자동으로 권한 범위가 조정되므로 유지보수가 용이합니다.\n",
      "Q. RAG에서 접근권한에 따라 답변은 어떻게 출력되는가?\n",
      "A.사용자가 검색이나 챗봇 질의를 수행할 경우, 시스템은 먼저 해당 사용자의 권한 정보를 확인한 후, 접근 가능한 문서만을 검색 대상으로 삼습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"청킹 전략에 대해 알려줘\"\n",
    "q_emb, _ = embed_batch([query])\n",
    "\n",
    "results = client_qd.query_points(\n",
    "    collection_name=\"lloydk_docs\",\n",
    "    query=q_emb[0],   # query_vector → query\n",
    "    limit=3,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "for r in results.points:\n",
    "    print(f\"score: {r.score:.3f}\")\n",
    "    print(f\"text: {r.payload['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0765d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 😊 오늘은 어떤 도움을 드릴까요?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': '안녕?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbcb00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 방금 성공하신 모델 이름으로 바꿨습니다!\n",
    "GEN_MODEL = 'gpt-oss:20b'\n",
    "\n",
    "def rag_chat():\n",
    "    print(f\"🤖 RAG 챗봇입니다. (사용 모델: {GEN_MODEL})\")\n",
    "    print(\"'exit' 입력 시 종료.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        q = input(\"👤 질문: \").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        if q.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            print(\"bye~\")\n",
    "            break\n",
    "\n",
    "        # 1️⃣ 쿼리 임베딩\n",
    "        # (앞서 만든 Qwen 임베딩 함수 호출)\n",
    "        try:\n",
    "            q_emb, _ = embed_batch([q])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 임베딩 에러: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2️⃣ Qdrant에서 검색\n",
    "        try:\n",
    "            results = client_qd.query_points(\n",
    "                collection_name=\"lloydk_docs\",\n",
    "                query=q_emb[0],\n",
    "                limit=5,\n",
    "                with_payload=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 검색 에러 (Qdrant가 켜져있나요?): {e}\")\n",
    "            continue\n",
    "\n",
    "        # 3️⃣ 검색된 문서 모으기\n",
    "        contexts = [r.payload[\"text\"] for r in results.points if \"text\" in r.payload]\n",
    "        \n",
    "        # 문서가 하나도 안 잡혔을 때 처리\n",
    "        if not contexts:\n",
    "            print(\"🤖 문서에서 관련 내용을 찾을 수 없어요. (검색 결과 0건)\")\n",
    "            # 검색 결과가 없어도 그냥 LLM에게 물어보고 싶다면 아래 continue를 지우세요.\n",
    "            continue \n",
    "\n",
    "        # 4️⃣ 답변 생성 (Ollama)\n",
    "        context_text = \"\\n\\n\".join(contexts)\n",
    "        \n",
    "        # 시스템 프롬프트 (성격을 부여)\n",
    "        system_prompt = (\n",
    "            \"당신은 기술 문서 검색 도우미입니다. \"\n",
    "            \"아래 제공된 [관련 문서]의 내용을 바탕으로 사용자의 질문에 한국어로 답변하세요. \"\n",
    "            \"문서에 없는 내용은 지어내지 말고, 모르면 모른다고 하세요.\"\n",
    "        )\n",
    "        \n",
    "        # 유저 프롬프트 (질문 + 검색된 문서 내용을 합쳐서 던짐)\n",
    "        user_prompt = (\n",
    "            f\"[질문]: {q}\\n\\n\"\n",
    "            f\"[관련 문서]:\\n{context_text}\\n\\n\"\n",
    "            \"위 내용을 바탕으로 답변해줘.\"\n",
    "        )\n",
    "\n",
    "        print(\"Thinking...\", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            # ✅ 방금 성공하신 코드 스타일 그대로 적용했습니다.\n",
    "            response = ollama.chat(\n",
    "                model=GEN_MODEL,\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': system_prompt},\n",
    "                    {'role': 'user', 'content': user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            print(\"\\r🤖 답변:                                \\n\")\n",
    "            print(response['message']['content'])\n",
    "            print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Ollama 에러: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef5787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 RAG 챗봇입니다. (사용 모델: gpt-oss:20b)\n",
      "'exit' 입력 시 종료.\n",
      "\n",
      "🤖 답변:                                \n",
      "\n",
      "팔란티어(Palantir)는 기업·정부·공공기관이 보유한 방대한 데이터를 **통합·분석·운영**할 수 있도록 하는 소프트웨어 플랫폼을 제공하는 회사입니다.  \n",
      "\n",
      "- **Foundry** : 기업 내부의 정형·비정형 데이터를 온톨로지(데이터·프로세스·의사결정 연계 모델)를 기반으로 통합하고, 시각화·분석·시뮬레이션까지 한 번에 수행할 수 있는 플랫폼입니다.  \n",
      "- **Gotham** : 주로 정부·국방·사이버보안 분야에서 활용되는 데이터 연결·추론·위협 분석 도구입니다.  \n",
      "- **AIP (AI Platform)** : LLM(대규모 언어모델)을 활용해 자연어 쿼리, 지능형 데이터 분석, 자동화된 업무 실행을 지원하는 AI 에이전트 플랫폼입니다.  \n",
      "\n",
      "팔란티어의 핵심은 “온톨로지 기반 데이터 해석”으로, 데이터 자체뿐 아니라 비즈니스 로직과 업무 프로세스까지 연결해 **실제 비즈니스 의사결정**을 지원합니다. 이를 통해 사일로화된 데이터를 통합하고, 분석·자동화·의사결정 과정을 한층 향상시킬 수 있습니다.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "bye~\n"
     ]
    }
   ],
   "source": [
    "rag_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
